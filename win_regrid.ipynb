{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78542ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c2228b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Author'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxr\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxesmf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxe\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiagnostics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProgressBar\n",
      "File \u001b[1;32mc:\\Users\\adamh\\miniconda3\\envs\\weather_env\\lib\\site-packages\\xesmf\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data, util\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrontend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Regridder, SpatialAverager\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\adamh\\miniconda3\\envs\\weather_env\\lib\\site-packages\\xesmf\\util.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiPolygon, Polygon\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mesmpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mESMF\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mESMF\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adamh\\miniconda3\\envs\\weather_env\\lib\\site-packages\\esmpy\\__init__.py:106\u001b[0m\n\u001b[0;32m    104\u001b[0m __requires_python__ \u001b[38;5;241m=\u001b[39m msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequires-Python\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# these don't seem to work with setuptools pyproject.toml\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m __author__ \u001b[38;5;241m=\u001b[39m \u001b[43mmsg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAuthor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    107\u001b[0m __homepage__ \u001b[38;5;241m=\u001b[39m msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHome-page\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    108\u001b[0m __obsoletes__ \u001b[38;5;241m=\u001b[39m msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobsoletes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\adamh\\miniconda3\\envs\\weather_env\\lib\\site-packages\\importlib_metadata\\_adapters.py:101\u001b[0m, in \u001b[0;36mMessage.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     99\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(item)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(item)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Author'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "from datetime import datetime\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask import delayed, compute\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress specific warning message from xesmf\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \n",
    "    message=\"Latitude is outside of \\\\[-90, 90\\\\]\", \n",
    "    module=\"xesmf.backend\"\n",
    ")\n",
    "\n",
    "def log(msg):\n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\")\n",
    "\n",
    "# Directories\n",
    "base_dir = \"D:\\\\training_data\"\n",
    "output_dir = \"D:\\\\training_data\\\\regridded_data\\\\\"\n",
    "climt_load = np.load('D:\\\\climt_lat_lon.npz')\n",
    "\n",
    "all_era5_level_files = sorted(glob.glob(os.path.join(base_dir, \"era5_model_levels_20*.nc\")))\n",
    "all_era5_level_z_files = sorted(glob.glob(os.path.join(base_dir, \"era5_model_levels_z_*.nc\")))\n",
    "all_era5_surface_files = sorted(glob.glob(os.path.join(base_dir, \"era5_surface_levels_*.nc\")))\n",
    "log(f\"Found {len(all_era5_level_files)} files to process.\")\n",
    "log(f\"Found {len(all_era5_level_z_files)} files to process.\")\n",
    "log(f\"Found {len(all_era5_surface_files)} files to process.\")\n",
    "\n",
    "\n",
    "# Open and process each year's data in chunks\n",
    "train_hours = [0, 6, 12, 18]\n",
    "test_hours = [1, 7, 13, 19]\n",
    "\n",
    "# load without chunking on valid_time, rechunk valid_time after loading\n",
    "log(f\"Starting merge for ds1...\")\n",
    "ds1 = xr.open_mfdataset(all_era5_level_files, engine=\"h5netcdf\", combine=\"by_coords\", parallel=True, chunks={'latitude': 32, 'longitude': 32, 'model_level': 32})\n",
    "ds1 = ds1.chunk({\"valid_time\": 100, \"latitude\": 32, \"longitude\": 32, \"model_level\": 32})\n",
    "log(f\"Done.\")\n",
    "log(f\"Starting merge for ds2...\")\n",
    "ds2 = xr.open_mfdataset(all_era5_level_z_files, engine=\"h5netcdf\", combine=\"by_coords\", parallel=True, chunks={'latitude': 32, 'longitude': 32, 'model_level': 32})\n",
    "ds2 = ds2.chunk({\"valid_time\": 100, \"latitude\": 32, \"longitude\": 32, \"model_level\": 32})\n",
    "log(f\"Done.\")\n",
    "log(f\"Starting merge for ds3...\")\n",
    "ds3 = xr.open_mfdataset(all_era5_surface_files, engine=\"h5netcdf\", combine=\"by_coords\", parallel=True, chunks={'latitude': 32, 'longitude': 32})\n",
    "ds3 = ds3.chunk({\"valid_time\": 100, \"latitude\": 32, \"longitude\": 32})\n",
    "log(f\"Done.\")\n",
    "# Assuming ds1, ds2, and ds3 have different variables but the same 'valid_time'\n",
    "log(f\"Starting merge for all...\")\n",
    "ds = xr.merge([ds1, ds2, ds3])\n",
    "log(f\"Merge complete.\")\n",
    "\n",
    "# Ensure 'valid_time' is in datetime format\n",
    "if \"valid_time\" in ds.coords and not ds[\"valid_time\"].dtype == \"datetime64[ns]\":\n",
    "    ds[\"valid_time\"] = ds[\"valid_time\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ffdc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ClimT grid\n",
    "climt_lat, climt_lon = climt_load['latitude'], climt_load['longitude']\n",
    "\n",
    "climt_grid = xr.Dataset(\n",
    "    {\n",
    "        'lat': (['lat'], climt_lat),\n",
    "        'lon': (['lon'], climt_lon),\n",
    "    }\n",
    ").chunk({\"lat\": 32, \"lon\": 32})\n",
    "\n",
    "def log(msg):\n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\")\n",
    "\n",
    "def xr_regrid(ds, target_grid):\n",
    "    chunk_dict = {\"valid_time\": 64, \"latitude\": 32, \"longitude\": 32}\n",
    "    if \"model_level\" in ds.dims:\n",
    "        chunk_dict[\"model_level\"] = 32\n",
    "\n",
    "    ds = ds.chunk(chunk_dict)\n",
    "\n",
    "    regridder = xe.Regridder(ds, target_grid, method='conservative', periodic=True)\n",
    "\n",
    "    regridded = regridder(ds, output_chunks={\"lat\": 32, \"lon\": 32})\n",
    "\n",
    "    if 'pressure_level' in regridded.coords:\n",
    "        regridded = regridded.assign_coords(\n",
    "            pressure_level=np.flip(regridded.pressure_level.values) * 100\n",
    "        )\n",
    "\n",
    "    return regridded.chunk({\"lat\": 32, \"lon\": 32, \"valid_time\": 64})\n",
    "\n",
    "# Train and test hours\n",
    "train_hours = [0, 6, 12, 18]\n",
    "test_hours = [1, 7, 13, 19]\n",
    "\n",
    "# Years to process\n",
    "years = range(2014, 2024)\n",
    "\n",
    "save_tasks = []\n",
    "\n",
    "log(\"Starting to process years...\")\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    log(f\"Processing year {year} ({i+1}/{len(years)})\")\n",
    "\n",
    "    ds_year = ds.sel(valid_time=ds.valid_time.dt.year == year)\n",
    "    total_steps = ds_year.valid_time.size\n",
    "    log(f\"  Total time steps: {total_steps}\")\n",
    "\n",
    "    ds_train = ds_year.sel(valid_time=ds_year.valid_time.dt.hour.isin(train_hours))\n",
    "    ds_test = ds_year.sel(valid_time=ds_year.valid_time.dt.hour.isin(test_hours))\n",
    "\n",
    "    valid_times_train = ds_train.valid_time.dt.floor('D').values\n",
    "    valid_times_test = ds_test.valid_time.dt.floor('D').values\n",
    "\n",
    "    unique_days = np.unique(np.concatenate([valid_times_train, valid_times_test]))\n",
    "\n",
    "    for dt in unique_days:\n",
    "        py_dt = dt.astype(\"M8[ms]\").astype(datetime)\n",
    "        y, m, d = py_dt.year, py_dt.month, py_dt.day\n",
    "\n",
    "        log(f\"Processing {y}-{m:02d}-{d:02d}...\")\n",
    "\n",
    "        ds_train_day = ds_train.sel(valid_time=ds_train.valid_time.dt.floor('D') == dt)\n",
    "        ds_test_day = ds_test.sel(valid_time=ds_test.valid_time.dt.floor('D') == dt)\n",
    "\n",
    "        if ds_train_day.valid_time.size == 0 and ds_test_day.valid_time.size == 0:\n",
    "            log(f\"  Skipping {y}-{m:02d}-{d:02d} (no data).\")\n",
    "            continue\n",
    "\n",
    "        ds_train_regrid = xr_regrid(ds_train_day, climt_grid)\n",
    "        ds_test_regrid = xr_regrid(ds_test_day, climt_grid)\n",
    "\n",
    "        train_day_dir = os.path.join(output_dir, \"train\", str(y), f\"{m:02d}\")\n",
    "        test_day_dir = os.path.join(output_dir, \"test\", str(y), f\"{m:02d}\")\n",
    "        os.makedirs(train_day_dir, exist_ok=True)\n",
    "        os.makedirs(test_day_dir, exist_ok=True)\n",
    "\n",
    "        train_day_path = os.path.join(train_day_dir, f\"{y}{m:02d}{d:02d}_train.nc\")\n",
    "        test_day_path = os.path.join(test_day_dir, f\"{y}{m:02d}{d:02d}_test.nc\")\n",
    "\n",
    "        # Delayed saving\n",
    "        save_train = delayed(ds_train_regrid.to_netcdf)(train_day_path)\n",
    "        save_test = delayed(ds_test_regrid.to_netcdf)(test_day_path)\n",
    "\n",
    "        save_tasks.extend([save_train, save_test])\n",
    "\n",
    "        # Trigger compute every 10 days\n",
    "        if len(save_tasks) >= 10:\n",
    "            log(\"Computing batch of 10 days...\")\n",
    "            compute(*save_tasks)\n",
    "            save_tasks.clear()\n",
    "\n",
    "# Final leftovers\n",
    "if save_tasks:\n",
    "    log(\"Computing final batch...\")\n",
    "    compute(*save_tasks)\n",
    "\n",
    "log(\"All data processed and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weather_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
