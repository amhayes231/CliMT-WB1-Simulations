{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0966caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import netron\n",
    "import xarray as xr\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd22927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Function to log messages with timestamps\n",
    "def log(msg):\n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daef23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-13 09:08:20] Starting data loading...\n",
      "[2025-04-13 09:08:20] Opening and combining NetCDF files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adamh/miniconda3/envs/xesmf_env/lib/python3.10/site-packages/dask/array/core.py:5097: PerformanceWarning: Increasing number of chunks by factor of 137\n",
      "  result = blockwise(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-13 09:09:55] Extracting temperature variable...\n",
      "[2025-04-13 09:09:55] Selecting model levels 61 to 137...\n",
      "[2025-04-13 09:09:55] Extracting valid_time hours...\n",
      "[2025-04-13 09:09:55] Filtering training times (00, 06, 12, 18)...\n",
      "[2025-04-13 09:10:05] Filtering testing times (01, 07, 13, 19)...\n",
      "[2025-04-13 09:10:16] Done.\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "log(\"Starting data loading...\")\n",
    "\n",
    "# Paths to your NetCDF files\n",
    "paths = \"/mnt/Elements/training_data/era5_model_levels_*.nc\"\n",
    "\n",
    "# Open and combine all files\n",
    "log(\"Opening and combining NetCDF files...\")\n",
    "ds = xr.open_mfdataset(paths, combine='by_coords')\n",
    "\n",
    "# Extract temperature\n",
    "log(\"Extracting temperature variable...\")\n",
    "temp = ds['t']  # adjust if needed\n",
    "\n",
    "# Select model levels 61 to 137\n",
    "log(\"Selecting model levels 61 to 137...\")\n",
    "temp = temp.isel(model_level=slice(-77, None))  #61 to 137\n",
    "\n",
    "# Extract the hour from time and filter\n",
    "log(\"Extracting valid_time hours...\")\n",
    "hours = temp['valid_time'].dt.hour\n",
    "\n",
    "# Training: 00, 06, 12, 18\n",
    "log(\"Filtering training times (00, 06, 12, 18)...\")\n",
    "train_hours = [0, 6, 12, 18]\n",
    "train = temp.sel(valid_time=temp['valid_time'].where(hours.isin(train_hours), drop=True))\n",
    "\n",
    "# Testing: 01, 07, 13, 19\n",
    "log(\"Filtering testing times (01, 07, 13, 19)...\")\n",
    "test_hours = [1, 7, 13, 19]\n",
    "test = temp.sel(valid_time=temp['valid_time'].where(hours.isin(test_hours), drop=True))\n",
    "\n",
    "log(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dddc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-13 09:11:07] Loading ClimT grid...\n",
      "[2025-04-13 09:11:07] Regridding training data...\n",
      "[2025-04-13 09:11:07] Creating regridder with weights file: /mnt/Elements/regrid_weights/bilinear_0def5f10f091d2681bf0f92676d969c7.nc\n",
      "[2025-04-13 09:11:20] Regridding entire dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adamh/miniconda3/envs/xesmf_env/lib/python3.10/site-packages/xarray/core/computation.py:315: PerformanceWarning: Regridding is increasing the number of chunks by a factor of 9.0, you might want to specify sizes in `output_chunks` in the regridder call. Default behaviour is to preserve the chunk sizes from the input (31, 60).\n",
      "  result_var = func(*data_vars)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-13 09:11:26] Regridding complete.\n",
      "[2025-04-13 09:11:26] Regridding testing data...\n",
      "[2025-04-13 09:11:26] Creating regridder with weights file: /mnt/Elements/regrid_weights/bilinear_0def5f10f091d2681bf0f92676d969c7.nc\n",
      "[2025-04-13 09:11:26] Regridding entire dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adamh/miniconda3/envs/xesmf_env/lib/python3.10/site-packages/xarray/core/computation.py:315: PerformanceWarning: Regridding is increasing the number of chunks by a factor of 9.0, you might want to specify sizes in `output_chunks` in the regridder call. Default behaviour is to preserve the chunk sizes from the input (31, 60).\n",
      "  result_var = func(*data_vars)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-13 09:11:38] Regridding complete.\n",
      "[2025-04-13 09:11:38] Process complete.\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import xesmf as xe\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "# Load ClimT grid\n",
    "log(\"Loading ClimT grid...\")\n",
    "climt_load = np.load('/run/media/adamh/X6/test_levels/climt_lat_lon.npz')\n",
    "climt_lat, climt_lon = climt_load['latitude'], climt_load['longitude']\n",
    "\n",
    "# Create a new grid for the regridding\n",
    "climt_grid = xr.Dataset(\n",
    "    {\n",
    "        'lat': (['lat'], climt_lat),\n",
    "        'lon': (['lon'], climt_lon),\n",
    "    }\n",
    ")\n",
    "\n",
    "def xr_regrid_all(ds, climt_grid, method='bilinear', weight_dir='/mnt/Elements/regrid_weights'):\n",
    "    \"\"\"\n",
    "    Regrid the entire dataset at once, without looping over individual time steps.\n",
    "    \"\"\"\n",
    "    # Create a unique hash for the grid config to name the weight file\n",
    "    grid_id = hashlib.md5((str(ds['latitude'].values.tobytes()) +\n",
    "                           str(ds['longitude'].values.tobytes()) +\n",
    "                           str(climt_grid['lat'].values.tobytes()) +\n",
    "                           str(climt_grid['lon'].values.tobytes()) +\n",
    "                           method).encode()).hexdigest()\n",
    "    \n",
    "    os.makedirs(weight_dir, exist_ok=True)\n",
    "    weight_path = os.path.join(weight_dir, f'{method}_{grid_id}.nc')\n",
    "\n",
    "    log(f\"Creating regridder with weights file: {weight_path}\")\n",
    "    regridder = xe.Regridder(ds, climt_grid, method=method, periodic=True,\n",
    "                             filename=weight_path,\n",
    "                             reuse_weights=os.path.exists(weight_path))\n",
    "\n",
    "    log(\"Regridding entire dataset...\")\n",
    "    regridded_ds = regridder(ds)\n",
    "\n",
    "    # Convert pressure levels (if present)\n",
    "    if 'pressure_level' in regridded_ds.coords:\n",
    "        regridded_ds = regridded_ds.assign_coords(\n",
    "            pressure_level=np.flip(regridded_ds.pressure_level.values) * 100\n",
    "        )\n",
    "\n",
    "    log(\"Regridding complete.\")\n",
    "    return regridded_ds\n",
    "\n",
    "# Regrid training and testing data\n",
    "log(\"Regridding training data...\")\n",
    "train_regridded = xr_regrid_all(train, climt_grid)\n",
    "\n",
    "log(\"Regridding testing data...\")\n",
    "test_regridded = xr_regrid_all(test, climt_grid)\n",
    "\n",
    "log(\"Process complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c90727",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleh = train_regridded.isel(valid_time=slice(0,5))\n",
    "print(bleh.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7436a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-13 09:15:54] Converting train_regridded...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert training and testing data\u001b[39;00m\n\u001b[1;32m     21\u001b[0m log(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting train_regridded...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m train_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_regridded\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Assuming 'train_regridded' is your training xarray\u001b[39;00m\n\u001b[1;32m     23\u001b[0m log(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting test_regridded...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m test_tensor \u001b[38;5;241m=\u001b[39m convert_to_tensor(test_regridded)  \u001b[38;5;66;03m# Assuming 'test_regridded' is your testing xarray\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(xr_data)\u001b[0m\n\u001b[1;32m      9\u001b[0m temp_data \u001b[38;5;241m=\u001b[39m xr_data  \u001b[38;5;66;03m# Replace 't' with the actual name of your variable if different\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Ensure the data is in the shape (77, 64, 128)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m temp_data \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m  \u001b[38;5;66;03m# This will give you a numpy array with shape (77, 64, 128)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(temp_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Add the batch dimension (1), making it (1, 77, 64, 128)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/xesmf_env/lib/python3.10/site-packages/xarray/core/dataarray.py:776\u001b[0m, in \u001b[0;36mDataArray.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    769\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;124;03m    The array's data as a numpy.ndarray.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;124;03m    type does not support coercion like this (e.g. cupy).\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xesmf_env/lib/python3.10/site-packages/xarray/core/variable.py:556\u001b[0m, in \u001b[0;36mVariable.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    555\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_as_array_or_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xesmf_env/lib/python3.10/site-packages/xarray/core/variable.py:306\u001b[0m, in \u001b[0;36m_as_array_or_item\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_array_or_item\u001b[39m(data):\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    it's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/xesmf_env/lib/python3.10/site-packages/dask/array/core.py:1748\u001b[0m, in \u001b[0;36mArray.__array__\u001b[0;34m(self, dtype, copy, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   1742\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1743\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt acquire a memory view of a Dask array. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1744\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will raise in the future.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   1746\u001b[0m     )\n\u001b[0;32m-> 1748\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;66;03m# Apply requested dtype and convert non-numpy backends to numpy.\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# If copy is True, numpy is going to perform its own deep copy\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;66;03m# after this method returns.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;66;03m# If copy is None, finalize() ensures that the returned object\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;66;03m# does not share memory with an object stored in the graph or on a\u001b[39;00m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;66;03m# process-local Worker.\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/xesmf_env/lib/python3.10/site-packages/dask/base.py:370\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/xesmf_env/lib/python3.10/site-packages/dask/base.py:649\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m args\n\u001b[1;32m    643\u001b[0m schedule \u001b[38;5;241m=\u001b[39m get_scheduler(\n\u001b[1;32m    644\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[1;32m    645\u001b[0m     collections\u001b[38;5;241m=\u001b[39mcollections,\n\u001b[1;32m    646\u001b[0m     get\u001b[38;5;241m=\u001b[39mget,\n\u001b[1;32m    647\u001b[0m )\n\u001b[0;32m--> 649\u001b[0m dsk \u001b[38;5;241m=\u001b[39m \u001b[43mcollections_to_dsk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m keys, postcomputes \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m collections:\n",
      "File \u001b[0;32m~/miniconda3/envs/xesmf_env/lib/python3.10/site-packages/dask/base.py:420\u001b[0m, in \u001b[0;36mcollections_to_dsk\u001b[0;34m(collections, optimize_graph, optimizations, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt, val \u001b[38;5;129;01min\u001b[39;00m groups\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    419\u001b[0m     dsk, keys \u001b[38;5;241m=\u001b[39m _extract_graph_and_keys(val)\n\u001b[0;32m--> 420\u001b[0m     dsk \u001b[38;5;241m=\u001b[39m \u001b[43mopt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m opt_inner \u001b[38;5;129;01min\u001b[39;00m optimizations:\n\u001b[1;32m    423\u001b[0m         dsk \u001b[38;5;241m=\u001b[39m opt_inner(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/xesmf_env/lib/python3.10/site-packages/dask/array/optimization.py:52\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(dsk, keys, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m dsk \u001b[38;5;241m=\u001b[39m optimize_blockwise(dsk, keys\u001b[38;5;241m=\u001b[39mkeys)\n\u001b[1;32m     51\u001b[0m dsk \u001b[38;5;241m=\u001b[39m fuse_roots(dsk, keys\u001b[38;5;241m=\u001b[39mkeys)\n\u001b[0;32m---> 52\u001b[0m dsk \u001b[38;5;241m=\u001b[39m \u001b[43mdsk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcull\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Perform low-level fusion unless the user has\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# specified False explicitly.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimization.fuse.active\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/xesmf_env/lib/python3.10/site-packages/dask/highlevelgraph.py:739\u001b[0m, in \u001b[0;36mHighLevelGraph.cull\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m    737\u001b[0m output_keys \u001b[38;5;241m=\u001b[39m keys_set\u001b[38;5;241m.\u001b[39mintersection(layer\u001b[38;5;241m.\u001b[39mget_output_keys())\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_keys:\n\u001b[0;32m--> 739\u001b[0m     culled_layer, culled_deps \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcull\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_ext_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;66;03m# Update `keys` with all layer's external key dependencies, which\u001b[39;00m\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;66;03m# are all the layer's dependencies (`culled_deps`) excluding\u001b[39;00m\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# the layer's output keys.\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     external_deps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/xesmf_env/lib/python3.10/site-packages/dask/highlevelgraph.py:158\u001b[0m, in \u001b[0;36mLayer.cull\u001b[0;34m(self, keys, all_hlg_keys)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m ret_deps[k]:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m seen:\n\u001b[0;32m--> 158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    159\u001b[0m             seen\u001b[38;5;241m.\u001b[39madd(d)\n\u001b[1;32m    160\u001b[0m             work\u001b[38;5;241m.\u001b[39madd(d)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def convert_to_tensor(temp_data):\n",
    "    \"\"\"\n",
    "    Convert regridded xarray data to a tensor suitable for U-Net input.\n",
    "    Assumes the xarray has shape (77, 64, 128) for temperature, with 77 being model levels.\n",
    "    \"\"\"\n",
    "    # Extract temperature (assuming the variable name is 't')\n",
    "    #temp_data = xr_data  # Replace 't' with the actual name of your variable if different\n",
    "\n",
    "    # Ensure the data is in the shape (77, 64, 128)\n",
    "    temp_data = temp_data.values  # This will give you a numpy array with shape (77, 64, 128)\n",
    "    print(temp_data.shape)\n",
    "\n",
    "    # Add the batch dimension (1), making it (1, 77, 64, 128)\n",
    "    temp_tensor = torch.tensor(temp_data).unsqueeze(0)  # Adds an extra dimension for the channel\n",
    "\n",
    "    return temp_tensor\n",
    "\n",
    "# Convert training and testing data\n",
    "log(\"Converting train_regridded...\")\n",
    "train_tensor = convert_to_tensor(train_regridded)  # Assuming 'train_regridded' is your training xarray\n",
    "log(\"Converting test_regridded...\")\n",
    "test_tensor = convert_to_tensor(test_regridded)  # Assuming 'test_regridded' is your testing xarray\n",
    "\n",
    "print(f\"Train tensor shape: {train_tensor.shape}\")\n",
    "print(f\"Test tensor shape: {test_tensor.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xesmf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
